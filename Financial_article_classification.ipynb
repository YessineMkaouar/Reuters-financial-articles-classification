{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reuters_sgm(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        data = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    articles = []\n",
    "\n",
    "    for reuters in soup.find_all('reuters'):\n",
    "\n",
    "        # Extract TITLE\n",
    "        title = reuters.find('title').text if reuters.find('title') else None\n",
    "\n",
    "        # Extract LEWISSPLIT attribute\n",
    "        lewissplit = reuters.get('lewissplit', 'UNKNOWN')\n",
    "\n",
    "        # Extract PLACES\n",
    "        places = [d.text for d in reuters.find('places').find_all('d')] if reuters.find('places') else []\n",
    "\n",
    "        # Extract DATE\n",
    "        date = reuters.find('date').text if reuters.find('date') else None\n",
    "\n",
    "        # Extract DATELINE\n",
    "        dateline = reuters.find('dateline').text if reuters.find('dateline') else None\n",
    "\n",
    "        # Extract BODY\n",
    "        body = reuters.find('text').body.text if reuters.find('text') and reuters.find('text').body else None\n",
    "\n",
    "        # Extract TOPICS\n",
    "        topics = [d.text for d in reuters.find('topics').find_all('d')] if reuters.find('topics') else []\n",
    "\n",
    "        # Append all extracted features to articles list\n",
    "        articles.append({\n",
    "            'TITLE': title,\n",
    "            'LEWISSPLIT': lewissplit,\n",
    "            'PLACES': places,\n",
    "            'DATE': date,\n",
    "            'DATELINE': dateline,\n",
    "            'BODY': body,\n",
    "            'TOPICS': topics\n",
    "        })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def process_all_sgm_files(directory_path):\n",
    "    \"\"\"\n",
    "    Process all .sgm files in a directory and combine the data into a single DataFrame.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.sgm'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            articles = parse_reuters_sgm(file_path)\n",
    "            all_articles.extend(articles)\n",
    "    \n",
    "    return pd.DataFrame(all_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the .sgm files\n",
    "directory = 'reuters21578'\n",
    "\n",
    "# Process all .sgm files and create a DataFrame\n",
    "df = process_all_sgm_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>LEWISSPLIT</th>\n",
       "      <th>PLACES</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DATELINE</th>\n",
       "      <th>BODY</th>\n",
       "      <th>TOPICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>ECOLAB &lt;ECL&gt; STARTS BID FOR CHEMLAWN &lt;CHEM&gt;</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>24-MAR-1987 08:07:26.91</td>\n",
       "      <td>NEW YORK, March 24 -</td>\n",
       "      <td>Ecolab Inc said it has started its\\npreviously...</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>JAPAN CUTTING CHINA CORN COMMITMENTS - USDA</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa, japan, china]</td>\n",
       "      <td>3-MAR-1987 17:40:09.18</td>\n",
       "      <td>WASHINGTON, March 3 -</td>\n",
       "      <td>Japanese traders have apparently\\nsharply redu...</td>\n",
       "      <td>[wheat, corn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15210</th>\n",
       "      <td>WALGREEN CO &lt;WAG&gt; 2ND QTR FEB 28 NET</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>8-APR-1987 13:57:29.01</td>\n",
       "      <td>DEERFIELD, Ill, April 8 -\\n</td>\n",
       "      <td>Shr 62 cts vs 58 cts\\n    Qtly div 13-1/2 cts ...</td>\n",
       "      <td>[earn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>HIGHER U.S. WEEKLY CAR OUTPUT ESTIMATED</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>26-MAR-1987 12:38:09.30</td>\n",
       "      <td>DETROIT, March 26 -</td>\n",
       "      <td>U.S. automakers are expected to build\\n167,236...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13637</th>\n",
       "      <td>None</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[]</td>\n",
       "      <td>7-APR-1987 08:31:45.50</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13723</th>\n",
       "      <td>None</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[]</td>\n",
       "      <td>7-APR-1987 09:25:24.41</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>&lt;ROYAL BANK OF CANADA&gt; 1ST QTR JAN 31 NET</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[canada]</td>\n",
       "      <td>3-MAR-1987 11:44:14.11</td>\n",
       "      <td>MONTREAL, March 3 -\\n</td>\n",
       "      <td>Shr basic 88 cts vs 1.22 dlrs\\n    Shr diluted...</td>\n",
       "      <td>[earn]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             TITLE LEWISSPLIT  \\\n",
       "8690   ECOLAB <ECL> STARTS BID FOR CHEMLAWN <CHEM>      TRAIN   \n",
       "1384   JAPAN CUTTING CHINA CORN COMMITMENTS - USDA      TRAIN   \n",
       "15210         WALGREEN CO <WAG> 2ND QTR FEB 28 NET       TEST   \n",
       "10020      HIGHER U.S. WEEKLY CAR OUTPUT ESTIMATED      TRAIN   \n",
       "13637                                         None      TRAIN   \n",
       "13723                                         None      TRAIN   \n",
       "1123     <ROYAL BANK OF CANADA> 1ST QTR JAN 31 NET      TRAIN   \n",
       "\n",
       "                    PLACES                     DATE  \\\n",
       "8690                 [usa]  24-MAR-1987 08:07:26.91   \n",
       "1384   [usa, japan, china]   3-MAR-1987 17:40:09.18   \n",
       "15210                [usa]   8-APR-1987 13:57:29.01   \n",
       "10020                [usa]  26-MAR-1987 12:38:09.30   \n",
       "13637                   []   7-APR-1987 08:31:45.50   \n",
       "13723                   []   7-APR-1987 09:25:24.41   \n",
       "1123              [canada]   3-MAR-1987 11:44:14.11   \n",
       "\n",
       "                                  DATELINE  \\\n",
       "8690                 NEW YORK, March 24 -    \n",
       "1384                WASHINGTON, March 3 -    \n",
       "15210      DEERFIELD, Ill, April 8 -\\n       \n",
       "10020                 DETROIT, March 26 -    \n",
       "13637                                 None   \n",
       "13723                                 None   \n",
       "1123             MONTREAL, March 3 -\\n       \n",
       "\n",
       "                                                    BODY         TOPICS  \n",
       "8690   Ecolab Inc said it has started its\\npreviously...          [acq]  \n",
       "1384   Japanese traders have apparently\\nsharply redu...  [wheat, corn]  \n",
       "15210  Shr 62 cts vs 58 cts\\n    Qtly div 13-1/2 cts ...         [earn]  \n",
       "10020  U.S. automakers are expected to build\\n167,236...             []  \n",
       "13637                                               None             []  \n",
       "13723                                               None             []  \n",
       "1123   Shr basic 88 cts vs 1.22 dlrs\\n    Shr diluted...         [earn]  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21578 entries, 0 to 21577\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   TITLE       20841 non-null  object\n",
      " 1   LEWISSPLIT  21578 non-null  object\n",
      " 2   PLACES      21578 non-null  object\n",
      " 3   DATE        21578 non-null  object\n",
      " 4   DATELINE    19043 non-null  object\n",
      " 5   BODY        19043 non-null  object\n",
      " 6   TOPICS      21578 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les articles sans body\n",
    "df = df.dropna(subset=['BODY'])\n",
    "\n",
    "# Supprimer les articles avec des topics = '[]'\n",
    "df = df[df['TOPICS'].apply(lambda x: x != [])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10377 entries, 0 to 21575\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   TITLE       10377 non-null  object\n",
      " 1   LEWISSPLIT  10377 non-null  object\n",
      " 2   PLACES      10377 non-null  object\n",
      " 3   DATE        10377 non-null  object\n",
      " 4   DATELINE    10377 non-null  object\n",
      " 5   BODY        10377 non-null  object\n",
      " 6   TOPICS      10377 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 648.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_topics = {'money-fx', 'ship', 'interest', 'acq', 'earn'}\n",
    "\n",
    "# Supprimer les articles qui ne contiennent pas les topics cibles\n",
    "df_cleaned = df[df['TOPICS'].apply(lambda x: bool(set(x) & target_topics))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_17872\\242027023.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['TOPICS'] = df_cleaned['TOPICS'].apply(lambda x: list(set(x) & target_topics)[0])\n"
     ]
    }
   ],
   "source": [
    "# Si un article contient plusieurs topics, on garde uniquement le topic présent dans target_topics\n",
    "df_cleaned['TOPICS'] = df_cleaned['TOPICS'].apply(lambda x: list(set(x) & target_topics)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>LEWISSPLIT</th>\n",
       "      <th>PLACES</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DATELINE</th>\n",
       "      <th>BODY</th>\n",
       "      <th>TOPICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6685</th>\n",
       "      <td>SERVICE CORP INTERNATIONAL &lt;SRV&gt; SETS QUARTERLY</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>18-MAR-1987 13:43:47.83</td>\n",
       "      <td>HOUSTON, March 18 -\\n</td>\n",
       "      <td>Qtly div eight cts vs eight cts prior\\n    Pay...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>SWAP DEALERS UNVEIL STANDARD CONTRACT</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[uk, usa]</td>\n",
       "      <td>4-MAR-1987 09:30:50.97</td>\n",
       "      <td>London, March 4 -</td>\n",
       "      <td>The International Swap Dealers\\nAssociation ha...</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16816</th>\n",
       "      <td>&lt;BIRDSBORO CORP&gt; 4TH QTR LOSS</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>17-APR-1987 09:24:56.94</td>\n",
       "      <td>MIAMI, April 17 -\\n</td>\n",
       "      <td>Shr loss 24 cts vs loss 20 cts\\n    Net loss 1...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>CONVENIENT FOOD MART &lt;CFMI&gt; AGREES TO BUY CHAIN</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>4-MAR-1987 10:52:39.54</td>\n",
       "      <td>ROSEMONT, Ill, March 4 -</td>\n",
       "      <td>Convenient Food Mart Inc said it\\nhas tentativ...</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15714</th>\n",
       "      <td>FIRST FEDERAL OF MICHIGAN &lt;FFOM&gt; 1ST QTR NET</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>9-APR-1987 12:22:10.55</td>\n",
       "      <td>DETROIT, April 9 -\\n</td>\n",
       "      <td>Shr 3.33 dlrs vs 3.39 dlrs\\n    Net 37,069,000...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>U.K. MONEY MARKET RECEIVES NO MORNING ASSISTANCE</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[uk]</td>\n",
       "      <td>17-MAR-1987 07:22:27.62</td>\n",
       "      <td>LONDON, March 17 -</td>\n",
       "      <td>The Bank of England said it did not\\noperate i...</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TITLE LEWISSPLIT     PLACES  \\\n",
       "6685    SERVICE CORP INTERNATIONAL <SRV> SETS QUARTERLY      TRAIN      [usa]   \n",
       "1420              SWAP DEALERS UNVEIL STANDARD CONTRACT      TRAIN  [uk, usa]   \n",
       "16816                     <BIRDSBORO CORP> 4TH QTR LOSS       TEST      [usa]   \n",
       "1505    CONVENIENT FOOD MART <CFMI> AGREES TO BUY CHAIN      TRAIN      [usa]   \n",
       "15714      FIRST FEDERAL OF MICHIGAN <FFOM> 1ST QTR NET       TEST      [usa]   \n",
       "5860   U.K. MONEY MARKET RECEIVES NO MORNING ASSISTANCE      TRAIN       [uk]   \n",
       "\n",
       "                          DATE                       DATELINE  \\\n",
       "6685   18-MAR-1987 13:43:47.83      HOUSTON, March 18 -\\n       \n",
       "1420    4-MAR-1987 09:30:50.97             London, March 4 -    \n",
       "16816  17-APR-1987 09:24:56.94        MIAMI, April 17 -\\n       \n",
       "1505    4-MAR-1987 10:52:39.54      ROSEMONT, Ill, March 4 -    \n",
       "15714   9-APR-1987 12:22:10.55       DETROIT, April 9 -\\n       \n",
       "5860   17-MAR-1987 07:22:27.62            LONDON, March 17 -    \n",
       "\n",
       "                                                    BODY    TOPICS  \n",
       "6685   Qtly div eight cts vs eight cts prior\\n    Pay...      earn  \n",
       "1420   The International Swap Dealers\\nAssociation ha...  interest  \n",
       "16816  Shr loss 24 cts vs loss 20 cts\\n    Net loss 1...      earn  \n",
       "1505   Convenient Food Mart Inc said it\\nhas tentativ...       acq  \n",
       "15714  Shr 3.33 dlrs vs 3.39 dlrs\\n    Net 37,069,000...      earn  \n",
       "5860   The Bank of England said it did not\\noperate i...  interest  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7175 entries, 8 to 21573\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   TITLE       7175 non-null   object\n",
      " 1   LEWISSPLIT  7175 non-null   object\n",
      " 2   PLACES      7175 non-null   object\n",
      " 3   DATE        7175 non-null   object\n",
      " 4   DATELINE    7175 non-null   object\n",
      " 5   BODY        7175 non-null   object\n",
      " 6   TOPICS      7175 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 448.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le lemmatiseur\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Retourne la catégorie grammaticale (POS) pour un mot.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatisation d'un texte en supprimant les stopwords.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens if word.isalnum()]\n",
    "\n",
    "def get_word_frequencies(texts, stop_words):\n",
    "    \"\"\"\n",
    "    Calcule les mots les plus fréquents à partir d'une liste de textes après lemmatisation.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            lemmatized_words = lemmatize_text(text)\n",
    "            words = [word for word in lemmatized_words if word not in stop_words]\n",
    "            all_words.extend(words)\n",
    "    return Counter(all_words).most_common(10)\n",
    "\n",
    "def compute_tfidf(texts, stop_words, top_n=10):\n",
    "    \"\"\"\n",
    "    Calcule les mots les plus significatifs selon TF-IDF après lemmatisation.\n",
    "    \"\"\"\n",
    "    if len(texts) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Appliquer la lemmatisation et supprimer les stopwords\n",
    "    processed_texts = [\n",
    "        ' '.join([word for word in lemmatize_text(text) if word not in stop_words])\n",
    "        for text in texts if isinstance(text, str)\n",
    "    ]\n",
    "    \n",
    "    # Initialiser le TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculer la moyenne des scores TF-IDF pour chaque mot\n",
    "    tfidf_avg_scores = tfidf_matrix.mean(axis=0).A1\n",
    "    tfidf_scores = list(zip(feature_names, tfidf_avg_scores))\n",
    "    \n",
    "    # Trier les mots par score TF-IDF\n",
    "    tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [word for word, _ in tfidf_scores[:top_n]]\n",
    "\n",
    "def descriptive_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyse descriptive pour identifier les caractéristiques principales de chaque sujet.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Stopwords à exclure\n",
    "    topic_analysis = {}\n",
    "\n",
    "    for topic in df['TOPICS'].unique():\n",
    "        topic_data = df[df['TOPICS'] == topic]\n",
    "        \n",
    "        # Longueur moyenne du texte\n",
    "        avg_body_length = topic_data['BODY'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0).mean()\n",
    "        \n",
    "        # Fréquence des mots dans BODY\n",
    "        body_frequencies = get_word_frequencies(topic_data['BODY'], stop_words)\n",
    "        \n",
    "        # Fréquence des mots dans TITLE\n",
    "        title_frequencies = get_word_frequencies(topic_data['TITLE'], stop_words)\n",
    "        \n",
    "        # TF-IDF pour BODY\n",
    "        body_tfidf = compute_tfidf(topic_data['BODY'].dropna().tolist(), stop_words)\n",
    "        \n",
    "        # TF-IDF pour TITLE\n",
    "        title_tfidf = compute_tfidf(topic_data['TITLE'].dropna().tolist(), stop_words)\n",
    "        \n",
    "        # Lieux les plus fréquents\n",
    "        places = [place for sublist in topic_data['PLACES'] for place in sublist]\n",
    "        places_frequencies = Counter(places).most_common(5)\n",
    "\n",
    "        # Stocker les résultats\n",
    "        topic_analysis[topic] = {\n",
    "            'Average BODY Length': avg_body_length,\n",
    "            'Top Words in BODY': body_frequencies,\n",
    "            'TF-IDF Words in BODY': body_tfidf,\n",
    "            'Top Words in TITLE': title_frequencies,\n",
    "            'TF-IDF Words in TITLE': title_tfidf,\n",
    "            'Top Places': places_frequencies\n",
    "        }\n",
    "    \n",
    "    return topic_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tMoney/Foreign Exchange (MONEY-FX)\n",
    "-\tShipping (SHIP)\n",
    "-\tInterest Rates (INTEREST)\n",
    "-\tMergers/Acquisitions (ACQ)\n",
    "-\tEarnings and Earnings Forecasts (EARN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: earn\n",
      "  Average BODY Length: 73.45887676337503\n",
      "  Top Words in BODY: [('v', 14004), ('mln', 11322), ('ct', 7894), ('dlrs', 5877), ('net', 5116), ('loss', 4648), ('shr', 4002), ('reuter', 3742), ('say', 3363), ('profit', 2997)]\n",
      "  TF-IDF Words in BODY: ['mln', 'ct', 'loss', 'dlrs', 'net', 'shr', 'rev', 'profit', 'reuter', 'say']\n",
      "  Top Words in TITLE: [('qtr', 1856), ('net', 1506), ('inc', 1149), ('corp', 750), ('4th', 671), ('loss', 517), ('3rd', 507), ('1st', 448), ('year', 443), ('31', 377)]\n",
      "  TF-IDF Words in TITLE: ['qtr', 'net', 'inc', 'corp', '4th', '3rd', 'loss', 'year', '1st', '31']\n",
      "  Top Places: [('usa', 3151), ('canada', 264), ('uk', 92), ('west-germany', 41), ('japan', 31)]\n",
      "\n",
      "Topic: acq\n",
      "  Average BODY Length: 128.77782805429865\n",
      "  Top Words in BODY: [('say', 7469), ('share', 3239), ('dlrs', 2828), ('company', 2820), ('mln', 2258), ('reuter', 2192), ('inc', 1913), ('pct', 1875), ('corp', 1482), ('offer', 1427)]\n",
      "  TF-IDF Words in BODY: ['say', 'share', 'dlrs', 'company', 'inc', 'mln', 'pct', 'corp', 'reuter', 'stock']\n",
      "  Top Words in TITLE: [('buy', 313), ('unit', 311), ('sell', 277), ('stake', 261), ('acquisition', 159), ('completes', 139), ('bid', 137), ('merger', 133), ('pct', 113), ('sale', 106)]\n",
      "  TF-IDF Words in TITLE: ['buy', 'unit', 'sell', 'stake', 'acquisition', 'completes', 'merger', 'bid', 'pct', 'make']\n",
      "  Top Places: [('usa', 1848), ('canada', 150), ('uk', 129), ('japan', 43), ('australia', 42)]\n",
      "\n",
      "Topic: ship\n",
      "  Average BODY Length: 167.7266435986159\n",
      "  Top Words in BODY: [('say', 1035), ('reuter', 288), ('gulf', 274), ('ship', 246), ('oil', 202), ('port', 188), ('iran', 184), ('would', 178), ('strike', 165), ('attack', 163)]\n",
      "  TF-IDF Words in BODY: ['say', 'gulf', 'port', 'ship', 'iran', 'strike', 'attack', 'iranian', 'oil', 'tonne']\n",
      "  Top Words in TITLE: [('gulf', 48), ('ship', 37), ('strike', 35), ('port', 30), ('say', 29), ('iran', 24), ('grain', 22), ('oil', 22), ('attack', 19), ('tanker', 16)]\n",
      "  TF-IDF Words in TITLE: ['gulf', 'ship', 'strike', 'say', 'grain', 'port', 'iran', 'oil', 'attack', 'freight']\n",
      "  Top Places: [('usa', 107), ('iran', 61), ('uk', 54), ('brazil', 26), ('kuwait', 24)]\n",
      "\n",
      "Topic: interest\n",
      "  Average BODY Length: 192.2759433962264\n",
      "  Top Words in BODY: [('say', 1621), ('rate', 1547), ('pct', 1260), ('bank', 1158), ('market', 654), ('interest', 437), ('reuter', 419), ('money', 365), ('billion', 365), ('would', 363)]\n",
      "  TF-IDF Words in BODY: ['rate', 'pct', 'bank', 'say', 'stg', 'mln', 'market', 'reserve', 'billion', 'prime']\n",
      "  Top Words in TITLE: [('rate', 171), ('bank', 88), ('market', 79), ('money', 76), ('cut', 63), ('fed', 60), ('prime', 53), ('say', 39), ('stg', 38), ('interest', 37)]\n",
      "  TF-IDF Words in TITLE: ['rate', 'bank', 'market', 'money', 'fed', 'cut', 'prime', 'reserve', 'add', 'pct']\n",
      "  Top Places: [('usa', 163), ('uk', 99), ('west-germany', 39), ('japan', 34), ('australia', 13)]\n",
      "\n",
      "Topic: money-fx\n",
      "  Average BODY Length: 218.779797979798\n",
      "  Top Words in BODY: [('say', 2364), ('bank', 1148), ('dollar', 1146), ('currency', 701), ('market', 683), ('rate', 675), ('exchange', 560), ('reuter', 491), ('yen', 489), ('pct', 469)]\n",
      "  TF-IDF Words in BODY: ['say', 'bank', 'dollar', 'stg', 'mln', 'rate', 'yen', 'market', 'currency', 'exchange']\n",
      "  Top Words in TITLE: [('say', 91), ('dollar', 90), ('market', 89), ('bank', 78), ('money', 72), ('japan', 66), ('currency', 60), ('mln', 40), ('yen', 40), ('stg', 39)]\n",
      "  TF-IDF Words in TITLE: ['market', 'dollar', 'say', 'bank', 'money', 'japan', 'currency', 'mln', 'stg', 'yen']\n",
      "  Top Places: [('usa', 177), ('japan', 149), ('uk', 104), ('west-germany', 78), ('france', 30)]\n"
     ]
    }
   ],
   "source": [
    "# Perform descriptive analysis\n",
    "analysis_results = descriptive_analysis(df_cleaned)\n",
    "\n",
    "# Display results\n",
    "for topic, details in analysis_results.items():\n",
    "    print(f\"\\nTopic: {topic}\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TOPICS\n",
       "earn        3757\n",
       "acq         2210\n",
       "money-fx     495\n",
       "interest     424\n",
       "ship         289\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['TOPICS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEWISSPLIT\n",
       "TRAIN       4984\n",
       "TEST        1977\n",
       "NOT-USED     214\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['LEWISSPLIT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 5198 articles\n",
      "Testing data: 1977 articles\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training ( 'Train' and 'not used' ) and testing sets\n",
    "train_data = df_cleaned[df_cleaned['LEWISSPLIT'].isin(['TRAIN', 'NOT-USED'])]\n",
    "test_data = df_cleaned[df_cleaned['LEWISSPLIT'] == 'TEST']\n",
    "\n",
    "print(f\"Training data: {train_data.shape[0]} articles\")\n",
    "print(f\"Testing data: {test_data.shape[0]} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environ 70% train et 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X_train = train_data['BODY']\n",
    "y_train = train_data['TOPICS']\n",
    "X_test = test_data['BODY']\n",
    "y_test = test_data['TOPICS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8730399595346484\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.77      0.98      0.86       643\n",
      "        earn       0.95      0.98      0.97      1042\n",
      "    interest       0.85      0.23      0.36       102\n",
      "    money-fx       0.81      0.46      0.59       105\n",
      "        ship       1.00      0.02      0.05        85\n",
      "\n",
      "    accuracy                           0.87      1977\n",
      "   macro avg       0.88      0.53      0.56      1977\n",
      "weighted avg       0.88      0.87      0.84      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that combines the TfidfVectorizer and the MultinomialNB classifier\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9539706626201315\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.94      0.98      0.96       643\n",
      "        earn       0.98      0.99      0.99      1042\n",
      "    interest       0.86      0.75      0.80       102\n",
      "    money-fx       0.79      0.77      0.78       105\n",
      "        ship       1.00      0.82      0.90        85\n",
      "\n",
      "    accuracy                           0.95      1977\n",
      "   macro avg       0.91      0.86      0.89      1977\n",
      "weighted avg       0.95      0.95      0.95      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline that combines the TfidfVectorizer and the LogisticRegression classifier\n",
    "logistic_model = make_pipeline(TfidfVectorizer(), LogisticRegression()) # Each row of the matrix (representing a document) is treated as a feature vector.\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9281740010116338\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.88      0.98      0.93       643\n",
      "        earn       0.98      0.99      0.98      1042\n",
      "    interest       0.88      0.65      0.75       102\n",
      "    money-fx       0.75      0.75      0.75       105\n",
      "        ship       1.00      0.41      0.58        85\n",
      "\n",
      "    accuracy                           0.93      1977\n",
      "   macro avg       0.90      0.75      0.80      1977\n",
      "weighted avg       0.93      0.93      0.92      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a pipeline that combines the TfidfVectorizer and the RandomForestClassifier\n",
    "random_forest_model = make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC (Support Vector Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9590288315629742\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.95      0.98      0.96       643\n",
      "        earn       0.98      0.99      0.99      1042\n",
      "    interest       0.88      0.77      0.82       102\n",
      "    money-fx       0.82      0.85      0.83       105\n",
      "        ship       1.00      0.80      0.89        85\n",
      "\n",
      "    accuracy                           0.96      1977\n",
      "   macro avg       0.93      0.88      0.90      1977\n",
      "weighted avg       0.96      0.96      0.96      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a pipeline that combines the TfidfVectorizer and the SVC classifier\n",
    "svc_model = make_pipeline(TfidfVectorizer(), SVC())\n",
    "\n",
    "# Train the model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Prédire les topics des articles ayant un body non vide et un topic non renseigné."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
