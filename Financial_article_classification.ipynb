{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reuters_sgm(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        data = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    articles = []\n",
    "\n",
    "    for reuters in soup.find_all('reuters'):\n",
    "\n",
    "        # Extract TITLE\n",
    "        title = reuters.find('title').text if reuters.find('title') else None\n",
    "\n",
    "        # Extract LEWISSPLIT attribute\n",
    "        lewissplit = reuters.get('lewissplit', 'UNKNOWN')\n",
    "\n",
    "        # Extract PLACES\n",
    "        places = [d.text for d in reuters.find('places').find_all('d')] if reuters.find('places') else []\n",
    "\n",
    "        # Extract DATE\n",
    "        date = reuters.find('date').text if reuters.find('date') else None\n",
    "\n",
    "        # Extract DATELINE\n",
    "        dateline = reuters.find('dateline').text if reuters.find('dateline') else None\n",
    "\n",
    "        # Extract BODY\n",
    "        body = reuters.find('text').body.text if reuters.find('text') and reuters.find('text').body else None\n",
    "\n",
    "        # Extract TOPICS\n",
    "        topics = [d.text for d in reuters.find('topics').find_all('d')] if reuters.find('topics') else []\n",
    "\n",
    "        # Append all extracted features to articles list\n",
    "        articles.append({\n",
    "            'TITLE': title,\n",
    "            'LEWISSPLIT': lewissplit,\n",
    "            'PLACES': places,\n",
    "            'DATE': date,\n",
    "            'DATELINE': dateline,\n",
    "            'BODY': body,\n",
    "            'TOPICS': topics\n",
    "        })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def process_all_sgm_files(directory_path):\n",
    "    \"\"\"\n",
    "    Process all .sgm files in a directory and combine the data into a single DataFrame.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.sgm'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            articles = parse_reuters_sgm(file_path)\n",
    "            all_articles.extend(articles)\n",
    "    \n",
    "    return pd.DataFrame(all_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the .sgm files\n",
    "directory = 'reuters21578'\n",
    "\n",
    "# Process all .sgm files and create a DataFrame\n",
    "df = process_all_sgm_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>LEWISSPLIT</th>\n",
       "      <th>PLACES</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DATELINE</th>\n",
       "      <th>BODY</th>\n",
       "      <th>TOPICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>ECOLAB &lt;ECL&gt; STARTS BID FOR CHEMLAWN &lt;CHEM&gt;</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>24-MAR-1987 08:07:26.91</td>\n",
       "      <td>NEW YORK, March 24 -</td>\n",
       "      <td>Ecolab Inc said it has started its\\npreviously...</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>JAPAN CUTTING CHINA CORN COMMITMENTS - USDA</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa, japan, china]</td>\n",
       "      <td>3-MAR-1987 17:40:09.18</td>\n",
       "      <td>WASHINGTON, March 3 -</td>\n",
       "      <td>Japanese traders have apparently\\nsharply redu...</td>\n",
       "      <td>[wheat, corn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15210</th>\n",
       "      <td>WALGREEN CO &lt;WAG&gt; 2ND QTR FEB 28 NET</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>8-APR-1987 13:57:29.01</td>\n",
       "      <td>DEERFIELD, Ill, April 8 -\\n</td>\n",
       "      <td>Shr 62 cts vs 58 cts\\n    Qtly div 13-1/2 cts ...</td>\n",
       "      <td>[earn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>HIGHER U.S. WEEKLY CAR OUTPUT ESTIMATED</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>26-MAR-1987 12:38:09.30</td>\n",
       "      <td>DETROIT, March 26 -</td>\n",
       "      <td>U.S. automakers are expected to build\\n167,236...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13637</th>\n",
       "      <td>None</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[]</td>\n",
       "      <td>7-APR-1987 08:31:45.50</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13723</th>\n",
       "      <td>None</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[]</td>\n",
       "      <td>7-APR-1987 09:25:24.41</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>&lt;ROYAL BANK OF CANADA&gt; 1ST QTR JAN 31 NET</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[canada]</td>\n",
       "      <td>3-MAR-1987 11:44:14.11</td>\n",
       "      <td>MONTREAL, March 3 -\\n</td>\n",
       "      <td>Shr basic 88 cts vs 1.22 dlrs\\n    Shr diluted...</td>\n",
       "      <td>[earn]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             TITLE LEWISSPLIT  \\\n",
       "8690   ECOLAB <ECL> STARTS BID FOR CHEMLAWN <CHEM>      TRAIN   \n",
       "1384   JAPAN CUTTING CHINA CORN COMMITMENTS - USDA      TRAIN   \n",
       "15210         WALGREEN CO <WAG> 2ND QTR FEB 28 NET       TEST   \n",
       "10020      HIGHER U.S. WEEKLY CAR OUTPUT ESTIMATED      TRAIN   \n",
       "13637                                         None      TRAIN   \n",
       "13723                                         None      TRAIN   \n",
       "1123     <ROYAL BANK OF CANADA> 1ST QTR JAN 31 NET      TRAIN   \n",
       "\n",
       "                    PLACES                     DATE  \\\n",
       "8690                 [usa]  24-MAR-1987 08:07:26.91   \n",
       "1384   [usa, japan, china]   3-MAR-1987 17:40:09.18   \n",
       "15210                [usa]   8-APR-1987 13:57:29.01   \n",
       "10020                [usa]  26-MAR-1987 12:38:09.30   \n",
       "13637                   []   7-APR-1987 08:31:45.50   \n",
       "13723                   []   7-APR-1987 09:25:24.41   \n",
       "1123              [canada]   3-MAR-1987 11:44:14.11   \n",
       "\n",
       "                                  DATELINE  \\\n",
       "8690                 NEW YORK, March 24 -    \n",
       "1384                WASHINGTON, March 3 -    \n",
       "15210      DEERFIELD, Ill, April 8 -\\n       \n",
       "10020                 DETROIT, March 26 -    \n",
       "13637                                 None   \n",
       "13723                                 None   \n",
       "1123             MONTREAL, March 3 -\\n       \n",
       "\n",
       "                                                    BODY         TOPICS  \n",
       "8690   Ecolab Inc said it has started its\\npreviously...          [acq]  \n",
       "1384   Japanese traders have apparently\\nsharply redu...  [wheat, corn]  \n",
       "15210  Shr 62 cts vs 58 cts\\n    Qtly div 13-1/2 cts ...         [earn]  \n",
       "10020  U.S. automakers are expected to build\\n167,236...             []  \n",
       "13637                                               None             []  \n",
       "13723                                               None             []  \n",
       "1123   Shr basic 88 cts vs 1.22 dlrs\\n    Shr diluted...         [earn]  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21578 entries, 0 to 21577\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   TITLE       20841 non-null  object\n",
      " 1   LEWISSPLIT  21578 non-null  object\n",
      " 2   PLACES      21578 non-null  object\n",
      " 3   DATE        21578 non-null  object\n",
      " 4   DATELINE    19043 non-null  object\n",
      " 5   BODY        19043 non-null  object\n",
      " 6   TOPICS      21578 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les articles sans body\n",
    "df = df.dropna(subset=['BODY'])\n",
    "\n",
    "# Supprimer les articles avec des topics = '[]'\n",
    "df = df[df['TOPICS'].apply(lambda x: x != [])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10377 entries, 0 to 21575\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   TITLE       10377 non-null  object\n",
      " 1   LEWISSPLIT  10377 non-null  object\n",
      " 2   PLACES      10377 non-null  object\n",
      " 3   DATE        10377 non-null  object\n",
      " 4   DATELINE    10377 non-null  object\n",
      " 5   BODY        10377 non-null  object\n",
      " 6   TOPICS      10377 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 648.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_topics = {'money-fx', 'ship', 'interest', 'acq', 'earn'}\n",
    "\n",
    "# Supprimer les articles qui ne contiennent pas les topics cibles\n",
    "df_cleaned = df[df['TOPICS'].apply(lambda x: bool(set(x) & target_topics))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_3288\\242027023.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['TOPICS'] = df_cleaned['TOPICS'].apply(lambda x: list(set(x) & target_topics)[0])\n"
     ]
    }
   ],
   "source": [
    "# Si un article contient plusieurs topics, on garde uniquement le topic présent dans target_topics\n",
    "df_cleaned['TOPICS'] = df_cleaned['TOPICS'].apply(lambda x: list(set(x) & target_topics)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>LEWISSPLIT</th>\n",
       "      <th>PLACES</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DATELINE</th>\n",
       "      <th>BODY</th>\n",
       "      <th>TOPICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6685</th>\n",
       "      <td>SERVICE CORP INTERNATIONAL &lt;SRV&gt; SETS QUARTERLY</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>18-MAR-1987 13:43:47.83</td>\n",
       "      <td>HOUSTON, March 18 -\\n</td>\n",
       "      <td>Qtly div eight cts vs eight cts prior\\n    Pay...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>SWAP DEALERS UNVEIL STANDARD CONTRACT</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[uk, usa]</td>\n",
       "      <td>4-MAR-1987 09:30:50.97</td>\n",
       "      <td>London, March 4 -</td>\n",
       "      <td>The International Swap Dealers\\nAssociation ha...</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16816</th>\n",
       "      <td>&lt;BIRDSBORO CORP&gt; 4TH QTR LOSS</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>17-APR-1987 09:24:56.94</td>\n",
       "      <td>MIAMI, April 17 -\\n</td>\n",
       "      <td>Shr loss 24 cts vs loss 20 cts\\n    Net loss 1...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>CONVENIENT FOOD MART &lt;CFMI&gt; AGREES TO BUY CHAIN</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>4-MAR-1987 10:52:39.54</td>\n",
       "      <td>ROSEMONT, Ill, March 4 -</td>\n",
       "      <td>Convenient Food Mart Inc said it\\nhas tentativ...</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15714</th>\n",
       "      <td>FIRST FEDERAL OF MICHIGAN &lt;FFOM&gt; 1ST QTR NET</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>9-APR-1987 12:22:10.55</td>\n",
       "      <td>DETROIT, April 9 -\\n</td>\n",
       "      <td>Shr 3.33 dlrs vs 3.39 dlrs\\n    Net 37,069,000...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>U.K. MONEY MARKET RECEIVES NO MORNING ASSISTANCE</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[uk]</td>\n",
       "      <td>17-MAR-1987 07:22:27.62</td>\n",
       "      <td>LONDON, March 17 -</td>\n",
       "      <td>The Bank of England said it did not\\noperate i...</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TITLE LEWISSPLIT     PLACES  \\\n",
       "6685    SERVICE CORP INTERNATIONAL <SRV> SETS QUARTERLY      TRAIN      [usa]   \n",
       "1420              SWAP DEALERS UNVEIL STANDARD CONTRACT      TRAIN  [uk, usa]   \n",
       "16816                     <BIRDSBORO CORP> 4TH QTR LOSS       TEST      [usa]   \n",
       "1505    CONVENIENT FOOD MART <CFMI> AGREES TO BUY CHAIN      TRAIN      [usa]   \n",
       "15714      FIRST FEDERAL OF MICHIGAN <FFOM> 1ST QTR NET       TEST      [usa]   \n",
       "5860   U.K. MONEY MARKET RECEIVES NO MORNING ASSISTANCE      TRAIN       [uk]   \n",
       "\n",
       "                          DATE                       DATELINE  \\\n",
       "6685   18-MAR-1987 13:43:47.83      HOUSTON, March 18 -\\n       \n",
       "1420    4-MAR-1987 09:30:50.97             London, March 4 -    \n",
       "16816  17-APR-1987 09:24:56.94        MIAMI, April 17 -\\n       \n",
       "1505    4-MAR-1987 10:52:39.54      ROSEMONT, Ill, March 4 -    \n",
       "15714   9-APR-1987 12:22:10.55       DETROIT, April 9 -\\n       \n",
       "5860   17-MAR-1987 07:22:27.62            LONDON, March 17 -    \n",
       "\n",
       "                                                    BODY    TOPICS  \n",
       "6685   Qtly div eight cts vs eight cts prior\\n    Pay...      earn  \n",
       "1420   The International Swap Dealers\\nAssociation ha...  interest  \n",
       "16816  Shr loss 24 cts vs loss 20 cts\\n    Net loss 1...      earn  \n",
       "1505   Convenient Food Mart Inc said it\\nhas tentativ...       acq  \n",
       "15714  Shr 3.33 dlrs vs 3.39 dlrs\\n    Net 37,069,000...      earn  \n",
       "5860   The Bank of England said it did not\\noperate i...  interest  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7175 entries, 8 to 21573\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   TITLE       7175 non-null   object\n",
      " 1   LEWISSPLIT  7175 non-null   object\n",
      " 2   PLACES      7175 non-null   object\n",
      " 3   DATE        7175 non-null   object\n",
      " 4   DATELINE    7175 non-null   object\n",
      " 5   BODY        7175 non-null   object\n",
      " 6   TOPICS      7175 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 448.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le lemmatiseur\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Retourne la catégorie grammaticale (POS) pour un mot.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatisation d'un texte en supprimant les stopwords.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens if word.isalnum()]\n",
    "\n",
    "def get_word_frequencies(texts, stop_words):\n",
    "    \"\"\"\n",
    "    Calcule les mots les plus fréquents à partir d'une liste de textes après lemmatisation.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            lemmatized_words = lemmatize_text(text)\n",
    "            words = [word for word in lemmatized_words if word not in stop_words]\n",
    "            all_words.extend(words)\n",
    "    return Counter(all_words).most_common(10)\n",
    "\n",
    "def compute_tfidf(texts, stop_words, top_n=10):\n",
    "    \"\"\"\n",
    "    Calcule les mots les plus significatifs selon TF-IDF après lemmatisation.\n",
    "    \"\"\"\n",
    "    if len(texts) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Appliquer la lemmatisation et supprimer les stopwords\n",
    "    processed_texts = [\n",
    "        ' '.join([word for word in lemmatize_text(text) if word not in stop_words])\n",
    "        for text in texts if isinstance(text, str)\n",
    "    ]\n",
    "    \n",
    "    # Initialiser le TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculer la moyenne des scores TF-IDF pour chaque mot\n",
    "    tfidf_avg_scores = tfidf_matrix.mean(axis=0).A1\n",
    "    tfidf_scores = list(zip(feature_names, tfidf_avg_scores))\n",
    "    \n",
    "    # Trier les mots par score TF-IDF\n",
    "    tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [word for word, _ in tfidf_scores[:top_n]]\n",
    "\n",
    "def descriptive_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyse descriptive pour identifier les caractéristiques principales de chaque sujet.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Stopwords à exclure\n",
    "    topic_analysis = {}\n",
    "\n",
    "    for topic in df['TOPICS'].unique():\n",
    "        topic_data = df[df['TOPICS'] == topic]\n",
    "        \n",
    "        # Longueur moyenne du texte\n",
    "        avg_body_length = topic_data['BODY'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0).mean()\n",
    "        \n",
    "        # Fréquence des mots dans BODY\n",
    "        body_frequencies = get_word_frequencies(topic_data['BODY'], stop_words)\n",
    "        \n",
    "        # Fréquence des mots dans TITLE\n",
    "        title_frequencies = get_word_frequencies(topic_data['TITLE'], stop_words)\n",
    "        \n",
    "        # TF-IDF pour BODY\n",
    "        body_tfidf = compute_tfidf(topic_data['BODY'].dropna().tolist(), stop_words)\n",
    "        \n",
    "        # TF-IDF pour TITLE\n",
    "        title_tfidf = compute_tfidf(topic_data['TITLE'].dropna().tolist(), stop_words)\n",
    "        \n",
    "        # Lieux les plus fréquents\n",
    "        places = [place for sublist in topic_data['PLACES'] for place in sublist]\n",
    "        places_frequencies = Counter(places).most_common(5)\n",
    "\n",
    "        # Stocker les résultats\n",
    "        topic_analysis[topic] = {\n",
    "            'Average BODY Length': avg_body_length,\n",
    "            'Top Words in BODY': body_frequencies,\n",
    "            'TF-IDF Words in BODY': body_tfidf,\n",
    "            'Top Words in TITLE': title_frequencies,\n",
    "            'TF-IDF Words in TITLE': title_tfidf,\n",
    "            'Top Places': places_frequencies\n",
    "        }\n",
    "    \n",
    "    return topic_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tMoney/Foreign Exchange (MONEY-FX)\n",
    "-\tShipping (SHIP)\n",
    "-\tInterest Rates (INTEREST)\n",
    "-\tMergers/Acquisitions (ACQ)\n",
    "-\tEarnings and Earnings Forecasts (EARN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: earn\n",
      "  Average BODY Length: 73.45887676337503\n",
      "  Top Words in BODY: [('v', 14004), ('mln', 11322), ('ct', 7894), ('dlrs', 5877), ('net', 5116), ('loss', 4648), ('shr', 4002), ('reuter', 3742), ('say', 3363), ('profit', 2997)]\n",
      "  TF-IDF Words in BODY: ['mln', 'ct', 'loss', 'dlrs', 'net', 'shr', 'rev', 'profit', 'reuter', 'say']\n",
      "  Top Words in TITLE: [('qtr', 1856), ('net', 1506), ('inc', 1149), ('corp', 750), ('4th', 671), ('loss', 517), ('3rd', 507), ('1st', 448), ('year', 443), ('31', 377)]\n",
      "  TF-IDF Words in TITLE: ['qtr', 'net', 'inc', 'corp', '4th', '3rd', 'loss', 'year', '1st', '31']\n",
      "  Top Places: [('usa', 3151), ('canada', 264), ('uk', 92), ('west-germany', 41), ('japan', 31)]\n",
      "\n",
      "Topic: acq\n",
      "  Average BODY Length: 128.77782805429865\n",
      "  Top Words in BODY: [('say', 7469), ('share', 3239), ('dlrs', 2828), ('company', 2820), ('mln', 2258), ('reuter', 2192), ('inc', 1913), ('pct', 1875), ('corp', 1482), ('offer', 1427)]\n",
      "  TF-IDF Words in BODY: ['say', 'share', 'dlrs', 'company', 'inc', 'mln', 'pct', 'corp', 'reuter', 'stock']\n",
      "  Top Words in TITLE: [('buy', 313), ('unit', 311), ('sell', 277), ('stake', 261), ('acquisition', 159), ('completes', 139), ('bid', 137), ('merger', 133), ('pct', 113), ('sale', 106)]\n",
      "  TF-IDF Words in TITLE: ['buy', 'unit', 'sell', 'stake', 'acquisition', 'completes', 'merger', 'bid', 'pct', 'make']\n",
      "  Top Places: [('usa', 1848), ('canada', 150), ('uk', 129), ('japan', 43), ('australia', 42)]\n",
      "\n",
      "Topic: ship\n",
      "  Average BODY Length: 167.7266435986159\n",
      "  Top Words in BODY: [('say', 1035), ('reuter', 288), ('gulf', 274), ('ship', 246), ('oil', 202), ('port', 188), ('iran', 184), ('would', 178), ('strike', 165), ('attack', 163)]\n",
      "  TF-IDF Words in BODY: ['say', 'gulf', 'port', 'ship', 'iran', 'strike', 'attack', 'iranian', 'oil', 'tonne']\n",
      "  Top Words in TITLE: [('gulf', 48), ('ship', 37), ('strike', 35), ('port', 30), ('say', 29), ('iran', 24), ('grain', 22), ('oil', 22), ('attack', 19), ('tanker', 16)]\n",
      "  TF-IDF Words in TITLE: ['gulf', 'ship', 'strike', 'say', 'grain', 'port', 'iran', 'oil', 'attack', 'freight']\n",
      "  Top Places: [('usa', 107), ('iran', 61), ('uk', 54), ('brazil', 26), ('kuwait', 24)]\n",
      "\n",
      "Topic: interest\n",
      "  Average BODY Length: 192.2759433962264\n",
      "  Top Words in BODY: [('say', 1621), ('rate', 1547), ('pct', 1260), ('bank', 1158), ('market', 654), ('interest', 437), ('reuter', 419), ('money', 365), ('billion', 365), ('would', 363)]\n",
      "  TF-IDF Words in BODY: ['rate', 'pct', 'bank', 'say', 'stg', 'mln', 'market', 'reserve', 'billion', 'prime']\n",
      "  Top Words in TITLE: [('rate', 171), ('bank', 88), ('market', 79), ('money', 76), ('cut', 63), ('fed', 60), ('prime', 53), ('say', 39), ('stg', 38), ('interest', 37)]\n",
      "  TF-IDF Words in TITLE: ['rate', 'bank', 'market', 'money', 'fed', 'cut', 'prime', 'reserve', 'add', 'pct']\n",
      "  Top Places: [('usa', 163), ('uk', 99), ('west-germany', 39), ('japan', 34), ('australia', 13)]\n",
      "\n",
      "Topic: money-fx\n",
      "  Average BODY Length: 218.779797979798\n",
      "  Top Words in BODY: [('say', 2364), ('bank', 1148), ('dollar', 1146), ('currency', 701), ('market', 683), ('rate', 675), ('exchange', 560), ('reuter', 491), ('yen', 489), ('pct', 469)]\n",
      "  TF-IDF Words in BODY: ['say', 'bank', 'dollar', 'stg', 'mln', 'rate', 'yen', 'market', 'currency', 'exchange']\n",
      "  Top Words in TITLE: [('say', 91), ('dollar', 90), ('market', 89), ('bank', 78), ('money', 72), ('japan', 66), ('currency', 60), ('mln', 40), ('yen', 40), ('stg', 39)]\n",
      "  TF-IDF Words in TITLE: ['market', 'dollar', 'say', 'bank', 'money', 'japan', 'currency', 'mln', 'stg', 'yen']\n",
      "  Top Places: [('usa', 177), ('japan', 149), ('uk', 104), ('west-germany', 78), ('france', 30)]\n"
     ]
    }
   ],
   "source": [
    "# Perform descriptive analysis\n",
    "analysis_results = descriptive_analysis(df_cleaned)\n",
    "\n",
    "# Display results\n",
    "for topic, details in analysis_results.items():\n",
    "    print(f\"\\nTopic: {topic}\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TOPICS\n",
       "earn        3766\n",
       "acq         2201\n",
       "money-fx     684\n",
       "ship         288\n",
       "interest     236\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['TOPICS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEWISSPLIT\n",
       "TRAIN       4984\n",
       "TEST        1977\n",
       "NOT-USED     214\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['LEWISSPLIT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 5198 articles\n",
      "Testing data: 1977 articles\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training ( 'Train' and 'not used' ) and testing sets\n",
    "train_data = df_cleaned[df_cleaned['LEWISSPLIT'].isin(['TRAIN', 'NOT-USED'])]\n",
    "test_data = df_cleaned[df_cleaned['LEWISSPLIT'] == 'TEST']\n",
    "\n",
    "print(f\"Training data: {train_data.shape[0]} articles\")\n",
    "print(f\"Testing data: {test_data.shape[0]} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environ 70% train et 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X_train = train_data['BODY']\n",
    "y_train = train_data['TOPICS']\n",
    "X_test = test_data['BODY']\n",
    "y_test = test_data['TOPICS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8730399595346484\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.77      0.98      0.86       643\n",
      "        earn       0.95      0.98      0.97      1042\n",
      "    interest       0.85      0.23      0.36       102\n",
      "    money-fx       0.81      0.46      0.59       105\n",
      "        ship       1.00      0.02      0.05        85\n",
      "\n",
      "    accuracy                           0.87      1977\n",
      "   macro avg       0.88      0.53      0.56      1977\n",
      "weighted avg       0.88      0.87      0.84      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that combines the TfidfVectorizer and the MultinomialNB classifier\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9539706626201315\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.94      0.98      0.96       643\n",
      "        earn       0.98      0.99      0.99      1042\n",
      "    interest       0.86      0.75      0.80       102\n",
      "    money-fx       0.79      0.77      0.78       105\n",
      "        ship       1.00      0.82      0.90        85\n",
      "\n",
      "    accuracy                           0.95      1977\n",
      "   macro avg       0.91      0.86      0.89      1977\n",
      "weighted avg       0.95      0.95      0.95      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline that combines the TfidfVectorizer and the LogisticRegression classifier\n",
    "logistic_model = make_pipeline(TfidfVectorizer(), LogisticRegression()) # Each row of the matrix (representing a document) is treated as a feature vector.\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9281740010116338\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.88      0.98      0.93       643\n",
      "        earn       0.98      0.99      0.98      1042\n",
      "    interest       0.88      0.65      0.75       102\n",
      "    money-fx       0.75      0.75      0.75       105\n",
      "        ship       1.00      0.41      0.58        85\n",
      "\n",
      "    accuracy                           0.93      1977\n",
      "   macro avg       0.90      0.75      0.80      1977\n",
      "weighted avg       0.93      0.93      0.92      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a pipeline that combines the TfidfVectorizer and the RandomForestClassifier\n",
    "random_forest_model = make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC (Support Vector Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9590288315629742\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.95      0.98      0.96       643\n",
      "        earn       0.98      0.99      0.99      1042\n",
      "    interest       0.88      0.77      0.82       102\n",
      "    money-fx       0.82      0.85      0.83       105\n",
      "        ship       1.00      0.80      0.89        85\n",
      "\n",
      "    accuracy                           0.96      1977\n",
      "   macro avg       0.93      0.88      0.90      1977\n",
      "weighted avg       0.96      0.96      0.96      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a pipeline that combines the TfidfVectorizer and the SVC classifier\n",
    "svc_model = make_pipeline(TfidfVectorizer(), SVC())\n",
    "\n",
    "# Train the model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got ['acq' 'earn' 'interest' 'money-fx' 'ship']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m make_pipeline(TfidfVectorizer(), XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Predict the topics for the test set\u001b[39;00m\n\u001b[0;32m     13\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1491\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1486\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1488\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1494\u001b[0m     )\n\u001b[0;32m   1496\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got ['acq' 'earn' 'interest' 'money-fx' 'ship']"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Create a pipeline that combines the TfidfVectorizer and the XGBoost classifier\n",
    "xgb_model = make_pipeline(TfidfVectorizer(), XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9640870005058169\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.98      0.98      0.98       643\n",
      "        earn       0.99      0.99      0.99      1042\n",
      "    interest       0.85      0.76      0.80       102\n",
      "    money-fx       0.76      0.86      0.81       105\n",
      "        ship       1.00      0.91      0.95        85\n",
      "\n",
      "    accuracy                           0.96      1977\n",
      "   macro avg       0.91      0.90      0.91      1977\n",
      "weighted avg       0.96      0.96      0.96      1977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create a pipeline that combines the TfidfVectorizer and the MLPClassifier\n",
    "mlp_model = make_pipeline(TfidfVectorizer(), MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42))\n",
    "\n",
    "# Train the model\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the topics for the test set\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Prédire les topics des articles ayant un body non vide et un topic non renseigné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the .sgm files\n",
    "directory = 'reuters21578'\n",
    "\n",
    "# Process all .sgm files and create a DataFrame\n",
    "dfb = process_all_sgm_files(directory)\n",
    "\n",
    "# Supprimer les articles sans body\n",
    "dfb = dfb.dropna(subset=['BODY'])\n",
    "\n",
    "# Étape 1 : Filtrer les articles sans topics\n",
    "missing_topics_dfb = dfb[dfb[\"TOPICS\"].apply(lambda x: x == [])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>LEWISSPLIT</th>\n",
       "      <th>PLACES</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DATELINE</th>\n",
       "      <th>BODY</th>\n",
       "      <th>TOPICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STANDARD OIL &lt;SRD&gt; TO FORM FINANCIAL UNIT</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>26-FEB-1987 15:02:20.00</td>\n",
       "      <td>CLEVELAND, Feb 26 -</td>\n",
       "      <td>Standard Oil Co and BP North America\\nInc said...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEXAS COMMERCE BANCSHARES &lt;TCB&gt; FILES PLAN</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>26-FEB-1987 15:03:27.51</td>\n",
       "      <td>HOUSTON, Feb 26 -</td>\n",
       "      <td>Texas Commerce Bancshares Inc's Texas\\nCommerc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TALKING POINT/BANKAMERICA &lt;BAC&gt; EQUITY OFFER</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa, brazil]</td>\n",
       "      <td>26-FEB-1987 15:07:13.72</td>\n",
       "      <td>LOS ANGELES, Feb 26 -</td>\n",
       "      <td>BankAmerica Corp is not under\\npressure to act...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RED LION INNS FILES PLANS OFFERING</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>26-FEB-1987 15:14:42.83</td>\n",
       "      <td>PORTLAND, Ore., Feb 26 -</td>\n",
       "      <td>Red Lion Inns Limited Partnership\\nsaid it fil...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>USX &lt;X&gt; DEBT DOWGRADED BY MOODY'S</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>26-FEB-1987 15:15:40.12</td>\n",
       "      <td>NEW YORK, Feb 26 -</td>\n",
       "      <td>Moody's Investors Service Inc said it\\nlowered...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21565</th>\n",
       "      <td>JAPAN STARTS CONSIDERING NOVEMBER BOND COUPON</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[japan, usa]</td>\n",
       "      <td>19-OCT-1987 02:01:46.85</td>\n",
       "      <td>TOKYO, Oct 19 -</td>\n",
       "      <td>The Ministry of Finance has started to\\nconsid...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21568</th>\n",
       "      <td>NEW ZEALAND NON-FUEL IMPORT ORDERS RISE IN AUGUST</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[new-zealand]</td>\n",
       "      <td>19-OCT-1987 01:45:38.39</td>\n",
       "      <td>WELLINGTON, Oct 19 -</td>\n",
       "      <td>Orders for non-fuel imports placed in\\nAugust ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21571</th>\n",
       "      <td>COMPETITION TOUGHENS FOR NEAR-SUPERCOMPUTERS</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>19-OCT-1987 01:14:35.36</td>\n",
       "      <td>BOSTON, Oct 19 -</td>\n",
       "      <td>An announcement by Alliant Computer\\nSystems I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21576</th>\n",
       "      <td>PROJECTIONS SHOW SWISS VOTERS WANT TRIED PARTIES</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[switzerland]</td>\n",
       "      <td>19-OCT-1987 00:03:21.69</td>\n",
       "      <td>BERNE, Oct 19 -</td>\n",
       "      <td>The prospect of a dominant alliance of\\nsocial...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21577</th>\n",
       "      <td>AMERICAN EXCHANGE INTRODUCES INSTITUTIONAL INDEX</td>\n",
       "      <td>TEST</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>19-OCT-1987 13:30:41.59</td>\n",
       "      <td>NEW YORK, Oct 19 -</td>\n",
       "      <td>The American Stock Exchange said it has\\nintro...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8666 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   TITLE LEWISSPLIT  \\\n",
       "1              STANDARD OIL <SRD> TO FORM FINANCIAL UNIT      TRAIN   \n",
       "2             TEXAS COMMERCE BANCSHARES <TCB> FILES PLAN      TRAIN   \n",
       "3           TALKING POINT/BANKAMERICA <BAC> EQUITY OFFER      TRAIN   \n",
       "6                     RED LION INNS FILES PLANS OFFERING      TRAIN   \n",
       "7                      USX <X> DEBT DOWGRADED BY MOODY'S      TRAIN   \n",
       "...                                                  ...        ...   \n",
       "21565      JAPAN STARTS CONSIDERING NOVEMBER BOND COUPON       TEST   \n",
       "21568  NEW ZEALAND NON-FUEL IMPORT ORDERS RISE IN AUGUST       TEST   \n",
       "21571       COMPETITION TOUGHENS FOR NEAR-SUPERCOMPUTERS       TEST   \n",
       "21576   PROJECTIONS SHOW SWISS VOTERS WANT TRIED PARTIES       TEST   \n",
       "21577   AMERICAN EXCHANGE INTRODUCES INSTITUTIONAL INDEX       TEST   \n",
       "\n",
       "              PLACES                     DATE                       DATELINE  \\\n",
       "1              [usa]  26-FEB-1987 15:02:20.00           CLEVELAND, Feb 26 -    \n",
       "2              [usa]  26-FEB-1987 15:03:27.51             HOUSTON, Feb 26 -    \n",
       "3      [usa, brazil]  26-FEB-1987 15:07:13.72         LOS ANGELES, Feb 26 -    \n",
       "6              [usa]  26-FEB-1987 15:14:42.83      PORTLAND, Ore., Feb 26 -    \n",
       "7              [usa]  26-FEB-1987 15:15:40.12            NEW YORK, Feb 26 -    \n",
       "...              ...                      ...                            ...   \n",
       "21565   [japan, usa]  19-OCT-1987 02:01:46.85               TOKYO, Oct 19 -    \n",
       "21568  [new-zealand]  19-OCT-1987 01:45:38.39          WELLINGTON, Oct 19 -    \n",
       "21571          [usa]  19-OCT-1987 01:14:35.36              BOSTON, Oct 19 -    \n",
       "21576  [switzerland]  19-OCT-1987 00:03:21.69               BERNE, Oct 19 -    \n",
       "21577          [usa]  19-OCT-1987 13:30:41.59            NEW YORK, Oct 19 -    \n",
       "\n",
       "                                                    BODY TOPICS  \n",
       "1      Standard Oil Co and BP North America\\nInc said...     []  \n",
       "2      Texas Commerce Bancshares Inc's Texas\\nCommerc...     []  \n",
       "3      BankAmerica Corp is not under\\npressure to act...     []  \n",
       "6      Red Lion Inns Limited Partnership\\nsaid it fil...     []  \n",
       "7      Moody's Investors Service Inc said it\\nlowered...     []  \n",
       "...                                                  ...    ...  \n",
       "21565  The Ministry of Finance has started to\\nconsid...     []  \n",
       "21568  Orders for non-fuel imports placed in\\nAugust ...     []  \n",
       "21571  An announcement by Alliant Computer\\nSystems I...     []  \n",
       "21576  The prospect of a dominant alliance of\\nsocial...     []  \n",
       "21577  The American Stock Exchange said it has\\nintro...     []  \n",
       "\n",
       "[8666 rows x 7 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_topics_dfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8666 entries, 1 to 21577\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   TITLE       8666 non-null   object\n",
      " 1   LEWISSPLIT  8666 non-null   object\n",
      " 2   PLACES      8666 non-null   object\n",
      " 3   DATE        8666 non-null   object\n",
      " 4   DATELINE    8666 non-null   object\n",
      " 5   BODY        8666 non-null   object\n",
      " 6   TOPICS      8666 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 541.6+ KB\n"
     ]
    }
   ],
   "source": [
    "missing_topics_dfb.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
